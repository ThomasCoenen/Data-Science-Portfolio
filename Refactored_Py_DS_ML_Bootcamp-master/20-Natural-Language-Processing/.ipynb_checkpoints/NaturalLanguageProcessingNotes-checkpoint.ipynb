{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Read Wiki aricle on NaturalLanguageProcessing\n",
    "#Free Datasets:\n",
    "https://archive.ics.uci.edu/ml/datasets.php\n",
    "\n",
    "NLP is its own Subgenre of ML!\n",
    "\n",
    "NLP serves as a lot of use cases when dealing with TEXT or UNSTRUCTURED TEXT Data. Imagine you work for Google news and you want to group News Articles by TOPIC. Or you work for a Legal Firm and you need to sift thorugh thousands of pages of legal documents to find relevant ones. \n",
    "#This is where NLP can help\n",
    "\n",
    "#We will want to \n",
    "-Compile Documents\n",
    "-Featurize Them\n",
    "-Compare their Features\n",
    "\n",
    "Example\n",
    "-You have 2 documents: (the are basically single sentences)\n",
    "  1-Blue House, 2-Red House\n",
    "-Features Text Docs Based on Word Count: (we want the ONES). FEATURIZE based off WORD COUNT and transform BLUE HOUSE into a VECTORIZED WORD COUNT. Create a VECTOR OCUNT of all posible words\n",
    "  1-Blue House - (red,blue,house) - (0,1,1)\n",
    "  2-Red House - (red,blue,house) - (1,0,1)\n",
    "#A DOCUMENT represented as a VECTOR of word counts is called a BAG OF WORDS\n",
    "#Once we have the BAG OF WORDS VECTORS we can use COSINE similarity on the vectors to determine similarity of the DOCMUEMNTS themselves\n",
    "#This is really useful bc we are treating each document as a VECTOR of FEAUTURES and we an perform MATH functions such as COSINE Similarity where we take the DOT PRODUCTS/MultofMagnitures, or other similarity metrics to find out how similar 2 text documents are to each other\n",
    "\n",
    "#We can improve BAG OF WORDS by adjusting word counts based on their frequency in CORPUS(the group of all the documents. \n",
    "#CORPUS - COLLECTION of TEXT is called a \n",
    "#We can use TF-IDF(Term Frequency - Inverse Documents Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TERM FREQUENCY (TF) - importance of the term within that documents \n",
    " -TF(d,t) = number of occuranes of term T in document D\n",
    " \n",
    "#INVERSE DOCUMENT FEQUNCY - importance of the term in the CORPUS\n",
    "  - IDF(t) - log(D/t) where:\n",
    "    -D = total number of documents\n",
    "    -t = number of ducuments with the term\n",
    "    \n",
    "Mathmatically TF-IDF is expressed as (term X in document Y): (see slides)\n",
    " #we do this to get a WORD COUNT and also some sort of NOTATION of how important a word is, not just relevant to the document but to the entire CORPUS of all the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In TERM:\n",
    "pip install nltk\n",
    "#NLTK allows you to work with Natural Language or \n",
    " #Text data very easy\n",
    "    \n",
    "#We will start out example by building a SPAM FILTER \n",
    "#with python and then your Portfo Proj will have you\n",
    "#working with REAL DATA from YELP an online review \n",
    " #site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP with Python - Part 1\n",
    "#Build a SPAM FILTER\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: d\n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "Hit Enter to continue: stopwords\n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "Hit Enter to continue: \n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "Hit Enter to continue: \n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [*] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [P] all-corpora......... All the corpora\n",
      "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [P] all................. All packages\n",
      "  [P] book................ Everything used in the NLTK Book\n",
      "  [P] popular............. Popular packages\n",
      "Hit Enter to continue: d\n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages; [P] marks partially installed collections)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> stopwords\n",
      "Command 'stopwords' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "#Download Necessary Data Set\n",
    "#NLTK comes with a bunch of data sets that are\n",
    "#necessary in order for it to operate in certain\n",
    "#ways\n",
    "nltk.download_shell()\n",
    "#tpye 'l' for LIST then 'ENTER', then shows you a \n",
    "#list of avaiable packages, 'ENTER' to continue\n",
    "#thru list, \"ENTER\" until you find STOPWORDS CORPUS,\n",
    "#type 'd' for DOWNLOAD, TYPE: STOPWORDS.\n",
    "\n",
    "#So this will download STOPWORDS for you\n",
    "\n",
    "#TYPE: 'q' to QUIT shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use DATASET from UCI DATASETS (see link in notebook)\n",
    "#THIS website has a ML Repository with a bunch of \n",
    " #Datasets we can use\n",
    "https://archive.ics.uci.edu/ml/index.php\n",
    "#We will be using SMS Spam Collection Dataset\n",
    "#its a bunch of Text(SMS data) classfied as SPAM or \n",
    " #HAM (which is a normal text). We will use data to \n",
    "    #build Spam Detection FILTER with Python\n",
    "#The file were using has more than 5000 SMS phone \n",
    " #messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "#Read data in\n",
    "#Using LIST COMPREHENSION\n",
    "#This is a TEXTEDIT file\n",
    "#STRIP open TEXT file to grab MESSAGES as a LIST of \n",
    " #MESSAGES\n",
    "messages = [line.rstrip() for line in \n",
    "    open('smsspamcollection/SMSSpamCollection')]\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "ham\tWhat you thinked about me. First time you saw me in class.\n",
      "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n"
     ]
    }
   ],
   "source": [
    "print(messages[0])\n",
    "#Check first message and we get STRING for the \n",
    " #FIRST message\n",
    "print(messages[50])\n",
    "print(messages[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "\n",
      "\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "\n",
      "\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "\n",
      "\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "\n",
      "\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#A COLLECTION of TEXT is called a CORPUS\n",
    "#Lets PRINT out first 10 MESSAGES and NUMBER them\n",
    " #using ENUMERATE\n",
    "for mess_no,message in enumerate(messages[:10]):\n",
    "    print(mess_no,message)\n",
    "    print('\\n')\n",
    "#Here we can clearly see STYLE of these TEXT \n",
    "#MESSAGES and some of them are clearly SPAM by \n",
    "#advertisements of FREE things, and other normal SPAM\n",
    "#emails asking you for money or claiming your a \n",
    "#WINNER\n",
    "\n",
    "#Due to the SPACING we can tell this is a TAB \n",
    "#SEPARATED VALUES FILE or TSV where the first COLUMN \n",
    "#is a LABEL saying whther message is normal or \n",
    "#not(ham or spam) and SECOND COL is MESSAGE iteself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can read in one of the lines\n",
    "messages[0]\n",
    "#Notice we have HAM, '\\t', and then the ACTUAL TEXT \n",
    "#MESAGE, so this indicates it a TAB SEPARATION\n",
    "#so we can use PANDAS to read this in as a CSV file\n",
    "\n",
    "#instead of just PARSING the TSV files manually \n",
    "#using python we can take advantage of PANDAS \n",
    "#knowledge to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Use READ_CSV and make note of the SCP Arguments\n",
    "#and we can also specify the desired COLUMN named \n",
    "#by passing in a LIST of NAMES\n",
    "#Names=names for columns=LABEL,MESSAGE\n",
    "messages = pd.read_csv('smsspamcollection/SMSSpamCollection',\n",
    "            sep='\\t',names=['label','message'])\n",
    "messages.head()\n",
    "#NOw we we have a nice DF with a Label and Message \n",
    "#String separated for us to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()\n",
    "#We see we have less unique messages vs the actual\n",
    "#message count whcih means we have repeated messages\n",
    "#in there which makes sense bc some may be common \n",
    "#text meesages like 'Hey'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ham</td>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>spam</td>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can use GROUPBY to use DESCRIBE METHOD by the \n",
    "#actual LABEL, and this way you can begin to think \n",
    "#about features that separate HAM and SPAM\n",
    "\n",
    "#GROUPBY LABEL Col\n",
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we continue we want to starting thinking about\n",
    "#the FEATURES were going to be using\n",
    "#With NLP a large part of this is FEATURE ENGINEERING\n",
    "#The better you domain knowledge on data the better\n",
    "#we can engineer features\n",
    "#FEATURE ENGINEERING is a large part of SPAM \n",
    "#detection in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make a new COLUMN to find HOW LONG TEXT \n",
    " #MESSAGES are\n",
    "#APPLY built in LENGTH function\n",
    "messages['length'] = messages['message'].apply(len)\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a20bf28d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASlklEQVR4nO3dfbBcdX3H8ffXBEFQiUDANA9eqBmEceShVxqLnVrRlgcl2AEL40hkoulMcdTqjF6sU3WmnQkzVsBphxrFNlAfeFAkJVSLAXT6h0AQCihQIqZwDSVRwkNFReTbP/a3vyzJTe7e5J7dm933a2Znz/md3+5+9+RwP5zfedjITCRJAnhRvwuQJM0choIkqTIUJEmVoSBJqgwFSVI1u98F7IlDDjkkR0ZG+l2GJO1V7rjjjp9l5tyJlu3VoTAyMsL69ev7XYYk7VUi4n92tszhI0lSZShIkipDQZJUGQqSpMpQkCRVhoIkqTIUJElVo6EQERsj4p6IuCsi1pe2gyLixoh4sDy/orRHRHwuIjZExN0RcXyTtUmSdtSLPYU/zsxjM3O0zI8B6zJzMbCuzAOcAiwujxXApT2oTZLUoR/DR0uB1WV6NXBGR/vl2fJ9YE5EzOt1cSNjaxkZW9vrj5WkGaHpUEjgPyLijohYUdoOy8xHAcrzoaV9PvBIx2vHS9sLRMSKiFgfEeu3bNnSYOmSNHyavvfRiZm5KSIOBW6MiPt30TcmaNvht0IzcxWwCmB0dLSx3xJt7y1sXHlaUx8hSTNOo3sKmbmpPG8GrgVOAB5rDwuV582l+ziwsOPlC4BNTdYnSXqhxkIhIg6IiJe1p4E/Ae4F1gDLSrdlwHVleg1wbjkLaQnwZHuYSZLUG00OHx0GXBsR7c/5SmZ+KyJuB66KiOXAw8BZpf8NwKnABuAZ4LwGa5MkTaCxUMjMh4BjJmj/OXDSBO0JnN9UPZKkyXlFsySpMhQkSZWhIEmqDAVJUmUoSJIqQ0GSVBkKkqTKUJAkVYaCJKkyFCRJlaEgSaoMBUlSZShMwp/nlDRMDAVJUmUoSJIqQ0GSVBkKkqTKUJAkVYaCJKkyFCRJlaEgSaoMBUlSZShIkipDQZJUGQqSpMpQkCRVhoIkqTIUJEmVoSBJqgwFSVJlKEiSKkNBklQ1HgoRMSsi7oyI68v84RFxa0Q8GBFXRsSLS/u+ZX5DWT7SdG2SpBfqxZ7CB4H7OuYvBC7KzMXAVmB5aV8ObM3MVwMXlX6SpB5qNBQiYgFwGvDFMh/Am4FrSpfVwBllemmZpyw/qfSXJPVI03sKFwMfBZ4v8wcDT2Tmc2V+HJhfpucDjwCU5U+W/i8QESsiYn1ErN+yZUuTtUvS0GksFCLibcDmzLyjs3mCrtnFsm0NmasyczQzR+fOnTsNlUqS2mY3+N4nAqdHxKnAfsDLae05zImI2WVvYAGwqfQfBxYC4xExGzgQeLzB+iRJ22lsTyEzL8jMBZk5ApwN3JSZ7wJuBs4s3ZYB15XpNWWesvymzNxhT0GS1Jx+XKfwMeDDEbGB1jGDy0r7ZcDBpf3DwFgfapOkodbk8FGVmbcAt5Tph4ATJujzK+CsXtQjSZqYVzRLkipDQZJUGQqSpMpQkCRVhoIkqTIUJEmVoSBJqgwFSVJlKHRpZGwtI2Nr+12GJDXKUJAkVYaCJKkyFCRJlaEgSaoMBUlSZShIkipDQZJU9eRHdgZJ57UKG1ee1sdKJGn6uacgSaoMhQHg1daSpouhIEmqDAVJUmUoSJIqQ0GSVBkKkqTKUJAkVYaCJKnqKhQi4rVNFyJJ6r9u9xT+KSJui4i/jIg5jVYkSeqbrkIhM98IvAtYCKyPiK9ExFsbrUyS1HNdH1PIzAeBTwAfA/4I+FxE3B8Rf9ZUcZKk3ur2mMLrIuIi4D7gzcDbM/OoMn1Rg/VJknqo21tn/wPwBeDjmfnLdmNmboqITzRSmSSp57odPjoV+Eo7ECLiRRGxP0BmXjHRCyJiv3Jw+r8i4ocR8enSfnhE3BoRD0bElRHx4tK+b5nfUJaP7OmXkyRNTbeh8B3gJR3z+5e2Xfk18ObMPAY4Fjg5IpYAFwIXZeZiYCuwvPRfDmzNzFfTGpK6sMvaJEnTpNtQ2C8z/689U6b339ULsqX9mn3KI2kdh7imtK8GzijTS8s8ZflJERFd1idJmgbdhsIvIuL49kxE/B7wy130b/ebFRF3AZuBG4EfA09k5nOlyzgwv0zPBx4BKMufBA6e4D1XRMT6iFi/ZcuWLsuXJHWj2wPNHwKujohNZX4e8OeTvSgzfwscWy54uxY4aqJu5XmivYLcoSFzFbAKYHR0dIflkqTd11UoZObtEfEa4Ehaf7zvz8zfdPshmflERNwCLAHmRMTssjewAGgHzTiti+PGI2I2cCDweNffRJK0x6ZyQ7zXA68DjgPOiYhzd9U5Iua2b4kRES8B3kLrOoebgTNLt2XAdWV6TZmnLL8pM90TkKQe6mpPISKuAH4XuAv4bWlO4PJdvGwesDoiZtEKn6sy8/qI+BHwtYj4W+BO4LLS/zLgiojYQGsP4eypfhlJ0p7p9pjCKHD0VP7PPTPvprVXsX37Q8AJE7T/Cjir2/eXJE2/boeP7gVe2WQhe6ORsbWMjK3tdxmSNG263VM4BPhRRNxG66I0ADLz9EaqkiT1Rbeh8Kkmi5AkzQzdnpL63Yh4FbA4M79T7ns0q9nSJEm91u2ts99H69YTny9N84FvNlWUJKk/uj3QfD5wIvAU1B/cObSpoiRJ/dFtKPw6M59tz5Qrjr2wTJIGTLeh8N2I+DjwkvLbzFcD/9ZcWZKkfug2FMaALcA9wF8AN9D6vWZJ0gDp9uyj52n9HOcXmi1HktRP3d776CdMfBvrI6a9IklS30zl3kdt+9G6R9FB01+OJKmfujqmkJk/73j8NDMvpvWzmpKkAdLt8NHxHbMvorXn8LJGKpIk9U23w0d/3zH9HLAReOe0VyNJ6qtuzz7646YLkST1X7fDRx/e1fLM/Oz0lCNJ6qepnH30elq/owzwduB7wCNNFCVJ6o+p/MjO8Zn5NEBEfAq4OjPf21RhkqTe6/Y2F4uAZzvmnwVGpr0aSVJfdbuncAVwW0RcS+vK5ncAlzdWlSSpL7o9++jvIuLfgT8sTedl5p3NlSVJ6oduh48A9geeysxLgPGIOLyhmiRJfdLtz3F+EvgYcEFp2gf416aKkiT1R7d7Cu8ATgd+AZCZm/A2F5I0cLoNhWczMym3z46IA5orSZLUL92GwlUR8XlgTkS8D/gO/uCOJA2cbs8++kz5beangCOBv8nMGxutTJLUc5OGQkTMAr6dmW8BDAJJGmCTDh9l5m+BZyLiwB7UI0nqo26vaP4VcE9E3Eg5AwkgMz/QSFWSpL7oNhTWlockaYDtMhQiYlFmPpyZq6f6xhGxkNb9kV4JPA+sysxLIuIg4EpaN9TbCLwzM7dGRACXAKcCzwDvycwfTPVzZ4KRsVZ+blx5Wp8rkaSpmeyYwjfbExHx9Sm+93PARzLzKGAJcH5EHA2MAesyczGwrswDnAIsLo8VwKVT/DxJ0h6abPgoOqaPmMobZ+ajwKNl+umIuA+YDywF3lS6rQZuoXULjaXA5eUiue9HxJyImFfeZ6/Q3kOQpL3VZKGQO5mekogYAY4DbgUOa/+hz8xHI+LQ0m0+L/wlt/HS9oJQiIgVtPYkWLRo0e6WNK0MA0mDYrLho2Mi4qmIeBp4XZl+KiKejoinuvmAiHgp8HXgQ5m5q9fEBG07BFFmrsrM0cwcnTt3bjclSJK6tMs9hcyctSdvHhH70AqEL2fmN0rzY+1hoYiYB2wu7ePAwo6XLwA27cnnDzr3UCRNt6n8nsKUlLOJLgPuy8zPdixaAywr08uA6zraz42WJcCTe9PxBEkaBN1ep7A7TgTeTeuit7tK28eBlbRusLcceBg4qyy7gdbpqBtonZJ6XoO1SZIm0FgoZOZ/MvFxAoCTJuifwPlN1SNJmlxjw0eSpL2PoSBJqgwFSVJlKEiSKkNBklQZCpKkylCQJFWGgiSpMhQkSZWhIEmqDIUeGBlb6x1NJe0VDAVJUtXkXVLVEPc6JDXFUGiQf7wl7W0cPpIkVYaCJKkyFCRJlaEgSaoMBUlSZShIkipDQZJUGQqSpMpQkCRVhoIkqTIUesi7pUqa6bz30V7EQJHUNENhL2AYSOoVh48kSZWhMEA8ZiFpTxkKkqTKUJAkVYaCJKlq7OyjiPgS8DZgc2a+trQdBFwJjAAbgXdm5taICOAS4FTgGeA9mfmDpmqbqbY/HrBx5Wl9qkTSsGpyT+FfgJO3axsD1mXmYmBdmQc4BVhcHiuASxusS5K0E42FQmZ+D3h8u+alwOoyvRo4o6P98mz5PjAnIuY1VZskaWK9PqZwWGY+ClCeDy3t84FHOvqNl7aB5KmjkmaqmXJFc0zQlhN2jFhBa4iJRYsWNVlT4yYLBoNDUq/1ek/hsfawUHneXNrHgYUd/RYAmyZ6g8xclZmjmTk6d+7cRouVpGHT61BYAywr08uA6zraz42WJcCT7WEmSVLvNHlK6leBNwGHRMQ48ElgJXBVRCwHHgbOKt1voHU66gZap6Se11RdkqSdaywUMvOcnSw6aYK+CZzfVC2SpO54RbMkqTIUJEmVoSBJqgwFSVJlKEiSKkNBklTNlNtc9J23lJAk9xQkSR0MBUlSZShIkipDQZJUGQqSpMpQkCRVhoIkqTIUJEmVoSBJqoY2FEbG1noVsyRtZ+hvc2EwSNI2Q7unIEnakaEgSaoMBUlSZShIkipDQZJUGQoDyNNtJe0uQ0GSVBkKkqTKUJAkVYbCEPAYg6RuDf1tLgaZQSBpqtxTkCRVhoIkqTIUJEmVoTCEtj/w7IFoSW0z6kBzRJwMXALMAr6YmSv7XNJA2f4P/2RBsLPlG1eeNm01SZpZZkwoRMQs4B+BtwLjwO0RsSYzf9TfyoZHOwQm+6O/fVh0ExKTBVATn9lL3a47aaabMaEAnABsyMyHACLia8BSwFDosakOJU3H0NPufmb7j/DO5iezsz/iOwuh3QnOyT6j30EyWR07Wz5T6m/CTP5uTdcWmdnIG09VRJwJnJyZ7y3z7wZ+PzPfv12/FcCKMnsk8MBufuQhwM9287WDxnWxjetiG9fFNoO2Ll6VmXMnWjCT9hRigrYdEiszVwGr9vjDItZn5uievs8gcF1s47rYxnWxzTCti5l09tE4sLBjfgGwqU+1SNJQmkmhcDuwOCIOj4gXA2cDa/pckyQNlRkzfJSZz0XE+4Fv0zol9UuZ+cMGP3KPh6AGiOtiG9fFNq6LbYZmXcyYA82SpP6bScNHkqQ+MxQkSdXQhUJEnBwRD0TEhogY63c9TYuIhRFxc0TcFxE/jIgPlvaDIuLGiHiwPL+itEdEfK6sn7sj4vj+foPpFxGzIuLOiLi+zB8eEbeWdXFlOdGBiNi3zG8oy0f6Wfd0i4g5EXFNRNxfto83DOt2ERF/Vf77uDcivhoR+w3rdjFUodBxK41TgKOBcyLi6P5W1bjngI9k5lHAEuD88p3HgHWZuRhYV+ahtW4Wl8cK4NLel9y4DwL3dcxfCFxU1sVWYHlpXw5szcxXAxeVfoPkEuBbmfka4Bha62TotouImA98ABjNzNfSOtHlbIZ1u8jMoXkAbwC+3TF/AXBBv+vq8Tq4jtb9pR4A5pW2ecADZfrzwDkd/Wu/QXjQuv5lHfBm4HpaF03+DJi9/TZC60y4N5Tp2aVf9Ps7TNN6eDnwk+2/zzBuF8B84BHgoPLvfD3wp8O4XWTmcO0psO0fv228tA2Fspt7HHArcFhmPgpQng8t3QZ9HV0MfBR4vswfDDyRmc+V+c7vW9dFWf5k6T8IjgC2AP9chtK+GBEHMITbRWb+FPgM8DDwKK1/5zsYzu1i6EKhq1tpDKKIeCnwdeBDmfnUrrpO0DYQ6ygi3gZszsw7Opsn6JpdLNvbzQaOBy7NzOOAX7BtqGgiA7suynGTpcDhwO8AB9AaLtveMGwXQxcKQ3krjYjYh1YgfDkzv1GaH4uIeWX5PGBzaR/kdXQicHpEbAS+RmsI6WJgTkS0L+Ts/L51XZTlBwKP97LgBo0D45l5a5m/hlZIDON28RbgJ5m5JTN/A3wD+AOGc7sYulAYultpREQAlwH3ZeZnOxatAZaV6WW0jjW0288tZ5ssAZ5sDyfs7TLzgsxckJkjtP7tb8rMdwE3A2eWbtuvi/Y6OrP0H4j/I8zM/wUeiYgjS9NJtG5TP3TbBa1hoyURsX/576W9LoZuuwCG60Bz+Xc7Ffhv4MfAX/e7nh583zfS2rW9G7irPE6lNQa6DniwPB9U+getM7R+DNxD64yMvn+PBtbLm4Dry/QRwG3ABuBqYN/Svl+Z31CWH9Hvuqd5HRwLrC/bxjeBVwzrdgF8GrgfuBe4Ath3WLcLb3MhSaqGbfhIkrQLhoIkqTIUJEmVoSBJqgwFSVJlKEiSKkNBklT9P2fdBrGprJakAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets visualize LENGTH of MESSAGE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Plot out LENGTH of Messages\n",
    "messages['length'].plot.hist(bins=150)\n",
    "#as you increase bin size we see BIMODAL behavior\n",
    "#(peak at lower end and peak at higher end)\n",
    "\n",
    "#We also see we have text lengths close to 1000 so\n",
    "#that means there a really long text message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.489950\n",
       "std        59.942907\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "messages['length'].describe()\n",
    "#So max Message length=910 which is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1085    For me the love should start with attraction.i...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets find this LONG MESSAGE with PANDAS MASKING\n",
    "messages[messages['length'] == 910]['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets find this LONG MESSAGE with PANDAS MASKING\n",
    "#USE ILOC to PRINT ENTIRE STRING\n",
    "messages[messages['length'] == 910]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x1a20e90cd0>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1a20f05e50>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets see if MESSAGE LENGTH is a DISTRINGUISHING \n",
    "#FEATURE between HAM and SPAM\n",
    "#Do something similar we did in SEABORN by specifying\n",
    " #the COLUMN. Meaning we can make SUBPLOTS based off\n",
    "    #the column(we did this in SEABORN with \n",
    "     #FACETGRID)but we will do it with PANDAS built\n",
    "        #in Data Viz\n",
    "messages.hist(column='length',by='label',bins=60,\n",
    "              figsize=(12,4))\n",
    "#Now we can see Pandas own version of FACET GRID,\n",
    "#Ham vs SPAM, 2 separate columns (length on X ax), \n",
    "#and it was separated BY the LABEL COLUMN\n",
    "\n",
    "#So SPAM MESSAGES TEND to have more characters\n",
    "#Ham-centered around 50 chars, Spam-centered around \n",
    " #150 chars\n",
    "    \n",
    "#So LENGTH is a good FEAUTRE to DISTINGUISH HAMM vs\n",
    "  #SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT PREPROCCESING METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NLP with Python - Part 2\n",
    "\n",
    "#LEarn about TEXT PREPROCCESSING since main issue with our data is its all in Text format or Strings.\n",
    "\n",
    "The Classifcation Algorithms we learned about need some Numerical Feature Vector in order to perform the CLASSIFCAITON TASKS. \n",
    "\n",
    "Need to Convert STRINGS to VECTOR Format and will use BAG OF WORDS to do this. For each unique word in the text will be represented by one number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will convert Raw Message (sequence of characters \n",
    "#as string) into VECTORS (a sequence of NUMBERS)\n",
    "#MESSAGE-VECTOR\n",
    "\n",
    "#Write Function that SPLITS MESSAGE into INDIVIDUAL \n",
    "#WORDS and RETURN a LIST. We will also remove very \n",
    "#common words like: the, a, and if(these are known as\n",
    "#STOP WORDS) and thats where were going to be using \n",
    "#NLTK library (thats why we downloaded STOPWORDS)\n",
    "\n",
    "#Also use Pythons Built in STRING Library\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove PUNCTUATION First\n",
    "#create sample message to show how it works\n",
    "mess = 'Sample message! Notice: it has punctuation.'\n",
    "string.punctuation\n",
    "#Get a string of various punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'a',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 'N',\n",
       " 'o',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use LIST COMPREGENSION to pass in for every \n",
    "#character and check if its not in the \n",
    " #STRING PUNCTUATION\n",
    "#So this will return only Characters and not \n",
    "  #Punctuation\n",
    "nopunc = [char for char in mess if char not in string.punctuation]\n",
    "nopunc\n",
    "#This is now the Elements in Original Message but \n",
    "#has removed and left as BLANK the punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets REMOVE STOP WORDS (common words)\n",
    "#import a list of ENGLISH STOPWORDS from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "#We get a LIST of all ENGLISH STOPWORDS. And we want\n",
    "#to remove them bc they dont tell you any \n",
    "#Distinguishing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample message Notice it has punctuation'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since NOPUNC is a LIST of letters\n",
    "#This is a way of JOINGING ELEMENTS in a LIST \n",
    " #together\n",
    "nopunc = ''.join(nopunc)\n",
    "nopunc\n",
    "#Now its back to its ORIGINAL STRING form but it \n",
    "#no longer has Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets look at whats going on with .JOIN\n",
    "#example string\n",
    "x = ['a','b','c','d']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use JOIN and pass in a LIST of ELEMENTS and it will\n",
    "#JOIN them and it will put in whatever String you put\n",
    "#in QUOTS BEFORE JOIN as their CONCATENATION DEVICE\n",
    "''.join(x)\n",
    "#So leaving a BLANK will JOIN everything together\n",
    "#So JOIN can be used to JOIN a List together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a++b++c++d'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'++'.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample message Notice it has punctuation'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'it', 'has', 'punctuation']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Back to assignment\n",
    "#SPLIT so we have a LIST\n",
    "nopunc.split()\n",
    "#now we have a LIST of all the words and we can now \n",
    "#use LIST COMPREHENSION to remove STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'punctuation']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use LIST COMPREHENSION to remove STOP WORDS\n",
    "clean_mess = [word for word in nopunc.split() \n",
    "    if word.lower() not in stopwords.words('english')]\n",
    "clean_mess\n",
    "#Now we just have important words (removed 'it' \n",
    " #and 'has')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets put these in a FUNCTION that we can apply to\n",
    " #entire DF\n",
    "#1- REMOVE PUNCTUATION - (makes into list), then JOIN \n",
    " #them back with JOIN method\n",
    "#2- REMOVE STOP WORDS\n",
    "#3- RETURN LIST of CLEANED TEXT WORDS\n",
    "    \n",
    "def text_process(mess):\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    \n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "#THIS funct is the same 3 steps we did earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will TOKENIZE the MESSAGE in the DF\n",
    "#TOKENIZATION - term used to describe the process of\n",
    "#what we just did. So convert a normal text string \n",
    "#into a LIST of TOKENS. TOKENS are the CLEANED WORDS\n",
    "#that we want\n",
    "\n",
    "#check out head again\n",
    "messages['message'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see the output on a column\n",
    "messages['message'].head(5).apply(text_process)\n",
    "#So we removed any STOP WORKDS and we create LIST of \n",
    "#the TOKENS(the important words we want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BE aware there lots of ways to NORMALIZE TEXT data.\n",
    "#Weve only done simple stuff on it. The NLTK library has lots of built in tools and docs on other methods of NORMALIZATION. \n",
    "For ex STEMMING is a common way to continue processing TEXT data. If. your text has a bunch of SIMILAR WORDS such as 'running' , 'ran', or 'run'. Basically these words are all same just differnent versions. STEMING breaks all these down and returns 'RUN'. The issue with STEMMING is you need a reference DICTIONARY to do this. And NLTK comes with a lot of those Built it datasets and references to do this easily\n",
    "\n",
    "#Some of these normalization methods can have trouble with SHORTHAND such as slang 'Nah' or 'U', so STEMMING wont work so great for our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#VECTORIZATION \n",
    "#Currenlty we have Messages as a LIST of TOKENS\n",
    "#now we need to convert each of the MESSAGES into a \n",
    "#VECTOR so ML Algoithms can understand\n",
    "\n",
    "3 STEPS:\n",
    "We'll do that in three steps using the bag-of-words model:\n",
    "Count how many times does a word occur in each message (Known as term frequency)\n",
    "Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "Normalize the vectors to unit length, to abstract from the original text length (L2 norm)    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#First Step\n",
    "#Count how many times does a word occur in each message\n",
    "#(Known as term frequency)\n",
    "#use COUNTVECTORIZER - this will convert a collection\n",
    " #of text documents to a MATRIX of TOKEN COUNTS (BAG\n",
    "    #of WORDS MODEL)\n",
    "We can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message.\n",
    "\n",
    "#So EACH COLUMN is ONE MESSAGE and the ROWS are the \n",
    " #INDIVIDUAL WORDS \n",
    "#So we will have a LARGE 2D MATRIX with ALL WORDS and ALL MESSAGES\n",
    "#Since theres so many Messages and Words we can expect we will have a lot of ZERO COUNTS for the presense of WORDS in that document, and because of this SKLEARN will OUTPUT what is known as a SPARSE MATRIX\n",
    "#SPARSE MATRIX is a way of dealing with a MATRIX that has a lot of ZERO VALUES.\n",
    "#So instead of STORING EVERY VALUE AND ELEMENT we can store as a SPARE MATRIX\n",
    "#So we will SAVE on a particular format to save the memory in out computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST STEP - CREATE THE MATRIX\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#There lots of Arguments that will be passed to \n",
    "#CountVectorizer and we will specify ANALYZER to be \n",
    "#our own previously defined function\n",
    "#BOW - bag of words transformer\n",
    "#We will specify our own Analyzer\n",
    "#FIT to TEXT DATA\n",
    "bow_transformer = CountVectorizer(analyzer=\n",
    "            text_process).fit(messages['message'])\n",
    "#so we just created a VERY LARGE MATRIX \n",
    "  #(may take a while to finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "#VOCALBULARY_ - prints total Number of VOCAB WORDS\n",
    "print(len(bow_transformer.vocabulary_))\n",
    "#So we hav 11425 WORDS in our VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "#We can take ONE SAMPLE TEXT MESSAGE and get its BAG \n",
    "#OF WORDS COUNT as a VECTOR putting to use our new \n",
    "#BowTransformer\n",
    "\n",
    "mess4 = messages['message'][3]\n",
    "print(mess4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4068)\t2\n",
      "  (0, 4629)\t1\n",
      "  (0, 5261)\t1\n",
      "  (0, 6204)\t1\n",
      "  (0, 6222)\t1\n",
      "  (0, 7186)\t1\n",
      "  (0, 9554)\t2\n",
      "(1, 11425)\n"
     ]
    }
   ],
   "source": [
    "#TRANSFORM and pass in MESSAGE as an ITEM in a LIST\n",
    "bow4 = bow_transformer.transform([mess4])\n",
    "print(bow4)\n",
    "print(bow4.shape)\n",
    "#So its ONE by our ENTIRE VOCABULARY. And this means \n",
    "#there are essentually 7 UNIQUE WORDS in MESSAGE 4 and \n",
    "#thats AFTER REMOVING COMMON STOP WORDS and 2 of them \n",
    "#appear TWICE and the REST only ONCE. We can check and \n",
    "#confirm which ones appear twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can check and confirm which ones appear TWICE\n",
    "#In BRACKETS: type at whatever Index occures Twice\n",
    "bow_transformer.get_feature_names()[4068]\n",
    "#So 'U' shows up twice and we saw that in the sentence\n",
    " #as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transformer.get_feature_names()[9554]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP with Python - Part 3\n",
    "#So in previous lecture we saw how many times \n",
    "#specific word counts show up in each message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 3)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also use .TRANSFORM METHOD on our \n",
    "#Bag Of Words Transformed Object and transform the \n",
    "#entire DF of MESSAGES\n",
    "\n",
    "#Lets see how BagOfWords counts for entire \n",
    "#CORPUS is a large sparsed Matrix\n",
    "messages_bow = bow_transformer.transform(messages['message'])\n",
    "#may take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (5572, 11425)\n"
     ]
    }
   ],
   "source": [
    "#Check SHAPE of BOW MATRIX\n",
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "#we have a 5572rows *11425cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50548"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also check the amount of NON ZERO occurances\n",
    "#use NNZ\n",
    "messages_bow.nnz\n",
    "#so we have 50548 non zero occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0.07940295412668218\n"
     ]
    }
   ],
   "source": [
    "#You can check SPARSITY by grabing a formula from\n",
    " #lecture notebook:\n",
    "sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))\n",
    "print('sparsity: {}'.format((sparsity)))\n",
    "#Sparsity =  100*Non Zero MEssages/Rows*Cols\n",
    "#We get 0, thats bc we have ROUND method and if we \n",
    "#delete it we see TRUE SPARSITY \n",
    "\n",
    "#SPARSITY COMPARES the number of NON ZEROS messages \n",
    "#vs the actual TOTAL number of messages. It gives you\n",
    "#an idea of how many ZEROS there are in your actual \n",
    "#MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n",
      "11425\n"
     ]
    }
   ],
   "source": [
    "print(messages_bow.shape[0]) #number of messages=5572\n",
    "print(messages_bow.shape[1]) #number of words=11425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9554)\t0.5385626262927564\n",
      "  (0, 7186)\t0.4389365653379857\n",
      "  (0, 6222)\t0.3187216892949149\n",
      "  (0, 6204)\t0.29953799723697416\n",
      "  (0, 5261)\t0.29729957405868723\n",
      "  (0, 4629)\t0.26619801906087187\n",
      "  (0, 4068)\t0.40832589933384067\n"
     ]
    }
   ],
   "source": [
    "#The term WEIGHT and NORMALIZAION can be done with\n",
    "#TFIDF(TERM FREQUENCY INVERSE DOCUMENT FREQUENCY)\n",
    "\n",
    "#We will use SKLEARN TFIDF TRANSFORMER OBJECT to do\n",
    " #that\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Make INSTANCE of TFID Transformer and FIT\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "\n",
    "#Fit BOW4\n",
    "tfidf4 = tfidf_transformer.transform(bow4)\n",
    "\n",
    "print(tfidf4)\n",
    "#We have an INVERSE DOCUMENT FREQUENCY, and a TERM \n",
    "# FREQUENCY Relationship for this particular message. \n",
    "# So we have been able to TRANSFORM a simple word \n",
    "# count into a TFIDF. \n",
    "\n",
    "#This can be interpreted as a WEIGHT value for each \n",
    "#of these words vs the ACTUAL DOCUMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.527076498901426"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check INVERSE DOCUMENT FREQUENCY of a PARTICULAR \n",
    "#WORD. For instance if we wanted to check Document \n",
    "#Frequency of word UNIVERSITY:\n",
    "\n",
    "#Pass in your Vocabulaty (BOW Transfomrer)\n",
    "tfidf_transformer.idf_[bow_transformer.vocabulary_['university']]\n",
    "#so this gives us IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets convert the ENTIRE BOW CORPUS into a TFIDF \n",
    "#CORPUS at once\n",
    "\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "\n",
    "#There are many the data can be preprocessed and \n",
    "#vectorized and these steps involve feature \n",
    "#engineering a building a pipline. Check documenation\n",
    "#when dealing with text data nad expansive collection\n",
    "#on NLP. This is a very large field and many paths to \n",
    "#take. Check out links in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN our MODEL\n",
    "\n",
    "#Now that we have MESSAGES represented as NUMERICAL \n",
    "#VECTORS we can TRAIN our SPAM HAM Classifier. We can \n",
    "#use any Classifcation Algorithm. The Naive Bays \n",
    "#Classifier Algorithm is a good choice\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Create an INSTANCE of Model and FIT\n",
    "spam_detect_model = MultinomialNB().fit(messages_tfidf,messages['label'])\n",
    "#SO WE TRAINED EVERYTHING ON TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype='<U4')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PREDICTIONS\n",
    "#Classify SINGLE RANDOM MESSAGE and check out how\n",
    " #we do\n",
    "#PREDICT of TFIDF4\n",
    "spam_detect_model.predict(tfidf4)\n",
    "#We just want first item in this output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get first item\n",
    "spam_detect_model.predict(tfidf4)[0]\n",
    "#detects that TFIDF4 MESSAGE will be HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets see if we predicted CORRECTLY \n",
    "messages['label'][3]\n",
    "#So we predicted CORRECTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'spam', ..., 'ham', 'ham', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If you want to run this PREDICTIION on ALL the \n",
    "#MESSAGES in our TFIDF:\n",
    "all_pred = spam_detect_model.predict(messages_tfidf)\n",
    "all_pred\n",
    "#Now we have Ham and Spam for ALL the PREDICTION of \n",
    " #ALL the MESSAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somethings thats important to note, is that we \n",
    "TRAINED everythign on our TRAINING DATA. So this \n",
    "above evaluation we evaluated stuff on the same data \n",
    "we used for TRAINING and we should NEVER do that we \n",
    "should be SPLITTING our DATA into a TEST/TRAINING \n",
    "SET, othersiwe we dont know the true preditive power \n",
    "of our model. If we rememeber each example during \n",
    "TRAINING the ACCURACY in TRIANING data would just be \n",
    "100% even though you wouldnt be able to classify any\n",
    "new messages. SO the proper way to do this is to use\n",
    "Train/Test SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3900 1672 5572\n"
     ]
    }
   ],
   "source": [
    "#TRAIN/TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X = FEAUTUES=MESSAGES themselves\n",
    "#y = LABEL(what we are predicting)\n",
    "msg_train, msg_test, label_train, label_test = \\\n",
    "train_test_split(messages['message'], messages['label'], test_size=0.3)\n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641      Pandy joined 4w technologies today.he got job..\n",
       "392     Hey so this sat are we going for the intro pil...\n",
       "5158    I will come with karnan car. Please wait till ...\n",
       "2184    Chinatown got porridge, claypot rice, yam cake...\n",
       "3356                        Minimum walk is 3miles a day.\n",
       "                              ...                        \n",
       "4961            I want  &lt;#&gt;  rs da:)do you have it?\n",
       "3096    Olol i printed out a forum post by a guy with ...\n",
       "987     I'm in office now . I will call you  &lt;#&gt;...\n",
       "5280                  Vikky, come around  &lt;TIME&gt; ..\n",
       "4369    1 I don't have her number and 2 its gonna be a...\n",
       "Name: message, Length: 3900, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_train\n",
    "#This is just LIST of all the TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 OPTIONS: \n",
    "1-we can grab TRIANING DATA(list of all the TEXT), and REPEAT everything we just did (CountVectorization, Transformation, TFIDF, FIT it ect) and then run MULTINOMIAL NB\n",
    "\n",
    "\n",
    "2-DATA PIPLINE FEATURE. We can run our model again and predict off the TEST SET by using SK LEARNS PIPLINES capabilities by storing an entire PIPLINE of our work flow. This is what we would do wit REAL WORLD TEXT DATA(you wont be doing all the steps we just did in previous 2 sections of this lecture series bc SK Learn has this which saves time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PIPLINE FEATURE\n",
    "#We will SUMMARIZE all the steps we just did into a \n",
    " #PIPELINE so we dont have to REPEAT eveyting for \n",
    "     #DIFFERENT SETS of data\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Make OBJECT which is an INSTANCE of PIPELINE\n",
    "#PIPLINE takes STEPS Argument(pass in a LIST of \n",
    " #everything you want to do)\n",
    "    \n",
    "#THE STEPS ARE EVERYTHIN WE DID ABOVE\n",
    "\n",
    "#PASS IN LIST OF TUPLES. And the TUPLES ahve 2 elements, \n",
    "  #the NAME of PROCESS, and ACTUL MODEL you want to pass\n",
    "    \n",
    "#First Step:Tuple-and the tuple takes in the NAME of\n",
    "#the Step, and what you want to do(first thing we did \n",
    "#was CountVectorizer with the ANALYZER=Text_Process \n",
    "#function - so this was STRING to TOKEN integer step \n",
    "#so STRING to TOKEN interger COUNT)\n",
    "\n",
    "#Second Step: TUPLE-take those INTEGER COUNTS to get \n",
    " #TFIDF SCORES\n",
    "    \n",
    "#3rd STEP: TUPLE-TRAIN on model. WE can use whatever\n",
    "#method we want as a CLASSIFIER(instead of MultNB).\n",
    "#we could have done RandonFORESTS\n",
    "\n",
    "#The Strings dont really matter, their just a label\n",
    " #for us to reference later\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_process)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier',MultinomialNB()) \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x1a214a0e60>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can directly pass message TEXT DATA and \n",
    "# PIPELINE will do all the preprocessing for us. You \n",
    "# basically teat Pipeline Model as normal ESTIMATOR\n",
    "\n",
    "#When we pass in MSG TRAIN and LABELTRAIN it does\n",
    " #the 3 steps for us in the PIPELINE, instead of us\n",
    "     #doing it manually ourselves\n",
    "#So this will FIT and TRAIN to all the data\n",
    "pipeline.fit(msg_train,label_train)\n",
    "#takes some time\n",
    "\n",
    "#We just have to pass in actual TEXT data, you no \n",
    "# longer have to manually doing CountVectoriation or \n",
    "# TFIDF transformation. \n",
    "\n",
    "#Once you have done this youve created a FITTED \n",
    " #PIPLINE OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTIONS\n",
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.97      1432\n",
      "        spam       1.00      0.69      0.82       240\n",
      "\n",
      "    accuracy                           0.96      1672\n",
      "   macro avg       0.98      0.85      0.90      1672\n",
      "weighted avg       0.96      0.96      0.95      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CLASSIFICATION REPORT\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(label_test,predictions,))\n",
    "#get a 96% ACCRUACY which is pretty good considering\n",
    "#we are just dealing with TEXT data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 3rd step use RANDOM FOREEST CLASSIFIER\n",
    " #(instead of MultNB) and see how compares\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_process)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier',RandomForestClassifier()) \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomascoenen/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x1a214a0e60>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=10, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTIONS\n",
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1432\n",
      "        spam       0.97      0.76      0.85       240\n",
      "\n",
      "    accuracy                           0.96      1672\n",
      "   macro avg       0.96      0.88      0.91      1672\n",
      "weighted avg       0.96      0.96      0.96      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CLASSIFICATION REPORT\n",
    "print(classification_report(label_test,predictions,))\n",
    "#Results are around the same but the F1 SCORE changed\n",
    "#for SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
