{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPARK - a late emerging technolgy for BIG DATA and one of\n",
    "the most pupulat libraires used for Python adn BigData\n",
    "\n",
    "#Learn: \n",
    "-HADOOP, MAPREDUCE, SPARK, and PYSPARK\n",
    "-Local vs Distributed Systems\n",
    "-overview of HADOOP Ecosystem\n",
    "-detailed overview of SPARK\n",
    "-Set us AmazonWebServices\n",
    "-Recoursec on other SPARK Options\n",
    "-Jupyter Notebook hands on code with PySpark and RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BIG DATA\n",
    "so far we've worked with data that can fit on a local computer in a scale of 0-8 gigs(data that fits direclty on our ram)\n",
    "#What do you do if you have a larger data set?\n",
    "-Try using SQL database to move storage onto hard drive instead of RAM(this will work on a local computer)\n",
    "-but if you need to expand beyong what LOCAL MACHINE CAN DO use a DISTRUTED SYSTEM that distributes the data to multiple machines/computers\n",
    "\n",
    "#a DISTRIBUTED SYSTEM - one machine costrol a distribution of multiple machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HADOOP - Lets Discuess format of DISTRIBUTED Architechure that uses HADOOP\n",
    "-HADOOP is a way to distribute very large files across multiple machines. \n",
    "-HADDOP it uses HDFS(HadoopDistributedFileSystem)\n",
    "-It allows us to work with LARGE DATA SETS\n",
    "-HDFS also duplicates blocks of data for fault tolerance\n",
    "-it also then used MapReduce\n",
    "-MapReduce allows computatiosn on that data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HFDS will use blocks of data with a size o 128MB by default. Each of the blocks is replicated 3 times. The blocks are distrubued in a way to support FAULT TOLERANCE, meaning if one of the lower data nodes get knocked out for some reason you have your data replicated on the other nodes, so you wont lose any information\n",
    "\n",
    "MapREDUCE is a way of splitting a computation task to a distributed set of files(such as HDFS)\n",
    "-it consists of a Job Tracker and multiple Task Trackers\n",
    "-Job Tracker sends code to run on Task Trackers\n",
    "-the Task Trackers allocate CPU and memory for the tasks and monitor the tasks on the worker nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#so what we covered can be summed up here:\n",
    "-using HDFS to distribute LARGE DATA SETS\n",
    "-Using MAP-REDUCE to distribute a computational task to a distributed data set\n",
    "NExt we will learn SPARK. SPARK impoves on the concepts of using DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SPARK OVERVIEW\n",
    "SPARK is one of the latest technologies being used to quickly and easily handle Big Data\n",
    "Its an open source projecy on APACHE\n",
    "SPARK is a flexible alternative to MAP-REDUCE\n",
    "\n",
    "Spark can use data stored in many formats:\n",
    "-CASSANDRA, AWS S3, HDFS, and more\n",
    "\n",
    "SPARK vs MAP-REDUCE\n",
    "-Spark is an alternative to MAP-Reduce but its not intended to replace HADOOP, but provide a solution to manage BIG DATA\n",
    "-MapReduce requires files to be stored in HDFS, SPARK does not!\n",
    "-SPARK also can perform operations up to 100x faster than MapReduce\n",
    "-MapReduce has to write thing to your disk or HardDrive, while SPARK can keep most of Data in RAM or Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of SPARK is the idea of RESILIENT DISTRIBUTED DATASET (RDD)\n",
    "RDD has 4 main feautures:\n",
    "-Distributed Collection of Data\n",
    "-Fault Tolerant\n",
    "-Parallel Operation-partioned\n",
    "-Ability to use many data sources\n",
    "\n",
    "RDD are immutable,lazily evaluated, and casheable\n",
    "\n",
    "There are 2 types of RDD oeprations(we will be coding out both of these):\n",
    "-TRANSFORMATIONS\n",
    "-ACTIONS\n",
    "\n",
    "Basic ACTIONS:\n",
    "-FIRST - return the first element in the RDD\n",
    "-COLLECT - returns all the elements of the RDD as an array at the driver program\n",
    "-COUNT - return the number of elements in the RDD\n",
    "-TAKE - Return an array with the first N elements of the RDD\n",
    "\n",
    "Basic TRANSFORMATIONS:\n",
    "-FILTER - applies a function to each element and returns elements that evaluate to true, very similar to python filter funct\n",
    "-MAP - Transforms each element and preserves number of elements, very similar to pandas.apply(), so map function to every element in RDD\n",
    "-FLATMAP - transforms each element into 0-N elements and probably changes number of elements from orignial RDD to after FlatMap operation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP vs FLATMAP\n",
    "-MAP - grabbing first letter of a list of names\n",
    "A MAP operation ex: you can have an RDD with a LIST of names you want to MAP an operation that grabs the first letter of each name, that way before and after the MAP operation, you are going to have same number of elements, one first letter for each name element\n",
    "\n",
    "-FLATMAP - Transforming a corpus of text into a list of words\n",
    "FlatMap example can be transforming a corpus of text into a list of words, so for instance in your first RDD each element is a line of text and then you can use FLATMAP to actually convert that into a list of words. Now each element is a singular word, so you will have more elements after FlatMap operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAIR RDDs\n",
    "-often RDD will be holding their values in TUPLES(key,value)\n",
    "-This offers better partisioning  of data and leads to functionality based on reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REDUCE METHOD\n",
    "-an action that will aggregate RDD elements using a function that returns a single element\n",
    "\n",
    "REDUCE BY KEY METHOD\n",
    "-an action that will aggregate PAIR RDD elemetns using a function that returns a PAIR RDD\n",
    "\n",
    "#both these methods are similar to GROUP BY OPERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS Account Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Set up AMAZON WEB SERVICES Acct to get SPARK up and running on the web\n",
    "#reason to use AWS is bc the whole idea of SPARK is you can no longer use LOCAL machine rather use a DISTRIBUTED SET of machines that you access thru CLOUD SERVICE\n",
    "\n",
    "#We will mainly work with AMAZON EC2. EC2 is basically our CLOUD computer \n",
    "#can only use 750 hours PER MONTH OR GET CHARGED. MAKE SURE TO SHUT DOWN ONCE DONE USING\n",
    "#NEVER LEAVE IT RUNNING\n",
    "\n",
    "#SIDE NOTE:\n",
    "When we set up the EC2 instance and configure the security groups setting for these spot instances we keep all the ports are open for simplicity, but it should be noted that these settings should be much more strict if put in production. So keep that in mind if you want to use the concepts discussed in the next lecture for a formal production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EC2 Instance Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#now we can create an EC2 INSTANCE\n",
    "AMAZON ELASTIC COMPUTE CLOUD(EC2) is a web service that provides resizable compute capcacity in the cloud.\n",
    "So it is basically a VIRTUAL COMPUTER we can access thru the internet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our PLAN:\n",
    "-Create EC2 INSTANCE on AWS\n",
    "-use SSH to connect to EC2 over internet\n",
    "-Set up SPARK and JUPYTER on EC2 INSTANCE\n",
    "\n",
    "SSH(Secure Shell Connection)\n",
    "-Our goal is to remotely connect to the TERMINAL of our \n",
    "virtual machine on Amazon EC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#START\n",
    "on AWS-SERVICES-COMPUTE-click on EC2\n",
    "\n",
    "Scroll down to Launch Instance and Click on it\n",
    "Click on UBUNTU Server\n",
    "\n",
    "#under FAMILY(these are all the instances we can create)\n",
    "Then we will use FREE TIER MICRO(its already checked)\n",
    "\n",
    "then click: Next Configure Instance Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brings us to 'CONFIGURE INSTANCE DETAILS' page\n",
    "Number Instances=1. As we mentioned earlier when dealing\n",
    "with spark or distriubted systems we may have a cluster \n",
    "of instances, when you are dealing with huge data you\n",
    "will put more than one instance here but we will use 1\n",
    "since its free\n",
    "#Everything else on default\n",
    "\n",
    "Click - Add Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brings us to 'Add Storage' page\n",
    "Keep default to 8 gigs\n",
    "\n",
    "Click: Next Tag Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brings us to 'NEXG TAG' page\n",
    "Set KEY: myspark\n",
    "Set Value: mymachine\n",
    "\n",
    "Click: Next Configure Security Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brings us to 'Configure Security Group' page\n",
    "Type: All Traffic\n",
    "    \n",
    "Click: REview and Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brings us to 'Review Instance Launch' page\n",
    "Click: Launch\n",
    "\n",
    "on Dropdown: Click: CREATE A NEW KEY PAIR\n",
    "        and under KEY PAIR NAME: type: newspark\n",
    "\n",
    "Click: Download Key Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you have to download the PRIVATE KEY FILE-its a .pem File\n",
    "YOU WONT BE ABLE TO DOWNLOAD THIS FILE AGAIN AFTER THIS\n",
    "WINDOW bc theres no way to come back to this. \n",
    "#PEM FILE NAME: NEWSPARK.PEM\n",
    "\n",
    "After download:\n",
    "Click: Launch Instances\n",
    " get: The following instance launches have been initiated: i-03a5888d531acbad9\n",
    "\n",
    "Click on Instance:\n",
    "#we see we have an. INTANCE ID and Public DNS and these\n",
    " #are the things we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we an perform ACTIONS\n",
    "most important ACITON: TERMINATE(under Instance State)\n",
    "#USE TERMINATE WHEN WE ARE DONE WITH INTSTANCE \n",
    "#AND DONT WANT TO GET CHARGED ANYMORE(we have 750 hrs per\n",
    "#month for whole year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO WE JUST CREATED OUR EC2 INSTANCE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSH with Mac or Linux\n",
    "#We will connect our INSTANCE thru our TERMINAL using SSH\n",
    "Move NEWSPARK.PEM to DESKTOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in TERM:\n",
    "    cd to desktop\n",
    "#use CH MOD command (change mod) to make sure our private key file \n",
    " #isn't publically viewable\n",
    "in TERM:\n",
    "    type: chmod 400 newspark.pem\n",
    "        #get: no output which means it worked\n",
    "\n",
    "in TERM:\n",
    " #Use SSH command to connect to our instance\n",
    " #grab PUBLIC DNS fron AWS CONSOLE: \n",
    "    Public DNS: ec2-3-21-236-144.us-east-2.compute.amazonaws.com\n",
    "    #mine is: ec2-13-59-87-187.us-east-2.compute.amazonaws.com   -OLD\n",
    "  type: ssh -i newspark.pem ubuntu@ec2-3-21-236-144.us-east-2.compute.amazonaws.com\n",
    "     #ec2-13-59-87-187.us-east-2.compute.amazonaws.com   --OLD\n",
    "\n",
    "  type 'yes' when asks if you want to continue\n",
    "    #Should say Ubuntu at some IP Addres - NOW WE ARE LOGGED IN\n",
    "  type: clear\n",
    "  #and right now we are in the Command Line interface to\n",
    "  #our EC2 Instance\n",
    "  #so we are running thru the CL a Virtual Connection to \n",
    "  #that computer (the EC2 Instance)\n",
    "  type: python3\n",
    "    #now we can run Python commands\n",
    "    print('hello')\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so we just made our UBUNTU INSTANCE\n",
    "#USE THIS TERMINAL INSTANCE TO DOWNLOAD EVERYTHING WE NEED to run SPARK\n",
    "#AND JUPYTER NOTEBOOK on an EC2 Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/Library/Frameworks/Python.framework/Versions/3.8/bin:${PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Setup\n",
    "set up Sparke Hadoop and Jupiter notebook all on our virtual instance \n",
    "that EC2 instance we created in the previous or earlier lectures.\n",
    "\n",
    "https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297\n",
    "\n",
    "We will Install a bunch of stuff thru TERM on our UBUNTU INSTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Login our SSH into that instance so you are at command line terminal\n",
    "for that ubuntu instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install ANACONDA on the Virtual Computer\n",
    "in TERM: type: wget https://repo.continuum.io/archive/Anaconda3-2018.12-Linux-x86_64.sh\n",
    "in TERM: type  bash Anaconda3-2018.12-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if everything is okay: Check which Python we are using\n",
    "#Ubuntu comes with Python but we want to make sure we are using Anaconda\n",
    "#Version\n",
    "in Ubuntu Term:\n",
    "    source .bashrc #anaconda asked us if we wanted to set bin in bashrc profile\n",
    "    \n",
    "source .bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anaconda3 will now be installed into this location:\n",
    "/home/ubuntu/anaconda3\n",
    "\n",
    "ERROR: File or directory already exists: '/home/ubuntu/anaconda3'\n",
    "If you want to update an existing installation, use the -u option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
