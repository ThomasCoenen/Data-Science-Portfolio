{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "https://docs.google.com/presentation/d/1WurfW8OWRqjiSmzmwOW71iN6CEShkbfOnyETqTvf6BE/edit#slide=id.g5c8d4edbeb_0_564\n",
    "\n",
    "book:\n",
    "http://faculty.marshall.usc.edu/gareth-james/ISL/\n",
    "\n",
    "Machine learning is a method of data analysis that automates analytical model building. \n",
    "Using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed where to look.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning algorithms are trained using labeled examples, such as an input where the desired output is known. \n",
    "For example, a segment of text could have a category label, such as:\n",
    "Spam vs. Legitimate Email\n",
    "Positive vs. Negative Movie Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Netork recieves INPUTS along with corresponding \n",
    "#correct OUTPUTS and alogorithm learns by\n",
    "#compating actual output with correct outputs to \n",
    "#find errors\n",
    "\n",
    "#SUPERVISED LEARNING is most used in applications\n",
    "werhe historical data predicts likely future events\n",
    "\n",
    "#When test model on Test Data get a performance \n",
    "  #metric:\n",
    " for Regression task: could be RootMeanSquared\n",
    " for Classification Task: could be Accuracy\n",
    "    \n",
    "    \n",
    "#Is it fair to use single split of data to evaluate\n",
    "our models performance? After all we are given chance\n",
    "to update model params over and over\n",
    "\n",
    "#Data in Nurel Networks and Deap Learning often\n",
    " #split into 3 sets:\n",
    "    Trainingg Data - used to train model params\n",
    "     so the model looks at features and correct\n",
    "        output and fit to this training data\n",
    "    \n",
    "    Validation Data - After Training on Training Data\n",
    "    we cheek performance on Val Data. Used to \n",
    "    determine what model hyperparameters to adjust\n",
    "    \n",
    "    Test Data - Used to get some final performance\n",
    "     metric. Once we run model thru Test Data this is\n",
    "        the performance metric we expect model to \n",
    "        perform with in real world, bc we dont go \n",
    "        back and adjust\n",
    "        \n",
    "  #In a nutshel: Train on Traiing Data to fit model. \n",
    "Use ValData to see how model performs on unseen data\n",
    "and then go back and ajust hyperparams. When it \n",
    "comes time to see how well model does in real world \n",
    "thats where TestData comes into play.\n",
    "\n",
    "#WE WILL JUST DO: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Performance -Classification Error Metrics\n",
    "#We will use PERFORMANE METRICS to evaluate how \n",
    " #our model did\n",
    "Key Classicfication Metrics:\n",
    " Accuracy-number of correct predictions made by the \n",
    "  model divided by the total number of predictions.\n",
    "    OR how many predictions did you get right as a \n",
    "     percentage?\n",
    "  ex: X_TEST was 100 images and our model correctly\n",
    "     predicted 80 images. so 80/100 = .8=80%\n",
    "  Accuracy is useful when target classes are well\n",
    "  balanced ie we have same amount of cat and dog \n",
    "     images, so labels are equally represented\n",
    "  WHAT IF UNBALANCED? - ACCURANCY IS NOT GOOD TO USE\n",
    "        \n",
    " Recall - ability of a model to find all the relevant\n",
    " cases with a dataset. REcall is the number of True\n",
    "    postives DIVIDED by the number of true positives\n",
    "    plus the number of false negatives\n",
    " \n",
    " Precision- ability of a classification model to \n",
    "    identify only the relevant data points. PRECISION\n",
    "    is the number of true positives divided by the \n",
    "    number of true positives plus the number of \n",
    "    false positives\n",
    "#Often there is a trade off between RECALL and \n",
    " #PRECISION: While Recall e xpresses ablility to find \n",
    " #all relevant instances in a dataset, precision \n",
    " #expresses the proportion of the data points our \n",
    "#model says was relecant actually were relevant\n",
    "\n",
    " F1-Score-essential a combo of Recall and Precision.\n",
    "    In cases where we want to find an optimal blend\n",
    "    of precision and recall we can combine the two\n",
    "    metrics using F1 Score. F1 Score is the harmonic\n",
    "    mean of all precision and recall taking both\n",
    "    metrics into account in eqn:\n",
    "        \n",
    "        F1 = 2*(precision*recall)/(precision+recall)\n",
    "    Using Harmonic Mean punishes extreme values\n",
    "    \n",
    "\n",
    "\n",
    "BINARY CLASSIFICATION - only have 2 available classes\n",
    " ZEROS and ONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX \n",
    " We can view all correctly classified verses \n",
    "incorrectly classified images in a confusion matrix\n",
    "\n",
    "True Positive - condition is ACTUALLY True AND model\n",
    " predicts the correct answer\n",
    "    \n",
    "True Negative - condition is ACTUALLY False AND model\n",
    " predicts correct answer\n",
    "    \n",
    "False Negative(type I error) - Model predicted False\n",
    " but it was actually true(condition positive)\n",
    "False Positive(type II error) - Model predicted True\n",
    " but it was actually negative(condition negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is we have UNBALANCED?\n",
    "#This is an UNBALANCED CLASS SITUATION ie we dont\n",
    " we don't 50%male and 50% female for a disase we are\n",
    "     evaluating \n",
    "This means we will have a PRECISION RECALL TRADOFF.\n",
    "We will need to decide if the model should focus on \n",
    "fixing False Positives vs False Negatives. In a \n",
    "disease diagnosis it s probly better to go in the \n",
    "direction of False Postives so we make sure we \n",
    "correctly classify as many cases of disease as \n",
    "possible. So at the cost of increasing False \n",
    "Positives we try our best to decrease False Negatives\n",
    "We want to make sure anyone who has disease that all\n",
    "those ppl that have disease go thru to the next \n",
    "step that is more invasive like a biopsy.\n",
    "What you really dont want is to turn someone away\n",
    "that has the disease that would be a False Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in our example we will predict if image is dog or \n",
    " #cat\n",
    "\n",
    "#Since this is SupervisedLearning, we will first\n",
    "FIT/TRAIN on TRAINING DATA, then TEST on TESTING DATA\n",
    "\n",
    "#Once we have model predictions from X_TEST DATA we \n",
    "compare it to the TRUE Y VALUES\n",
    "\n",
    "#TEST DATA- this is where we EVALUATE MODELS \n",
    "PERFORMACNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating Performance - Regression Error Metrics\n",
    "\n",
    "#Regression is a task when a model attempts to \n",
    " predict continous values(unlike categorical values,\n",
    " which is classification)\n",
    "#Accuracy and Recall are NOT USEFULL for REGRESSION\n",
    "since we need metrics designed for CONTINUOUS\n",
    "values\n",
    "#For ex attempting to predict price of a house \n",
    "given its features is a REGRESSION TASK\n",
    "\n",
    "#for ex attempting to predict the country  a house is\n",
    "is given its features is a CLASSIFICATION task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most common evaluation metrics for REGRESSION\n",
    "Mean Absolute Error - mean of the absolute value of\n",
    " errors. Compare our predictions to the true Y-label.\n",
    " For ex compare the Prediction of House Price with\n",
    " True House Price. \n",
    "    abs(yi - yhat)\n",
    "    yi=true price\n",
    "    yhat = predicted price\n",
    "MAE WONT punish LARGE errors for ex if you have a \n",
    " HUGE OUTLIER. So large errors are noted more with\n",
    "     MAE\n",
    "\n",
    "Mean Squared Error - if you have HUGE OUTLIER this is\n",
    " better. This is the MEAN of the squared errors.\n",
    "    (yi-yhat)^2\n",
    " The larger errors are noted MORE than with MAE \n",
    " which makes MSE more popular bc you punish model\n",
    "    more for large outliers. \n",
    "\n",
    "\n",
    "Root Mean Square Error - Most poplular. This is the \n",
    " root of the MSE. In punishes large errors and has \n",
    "    same units as y\n",
    "  \n",
    "\n",
    "#Compare out ERROR METRIC to the average value of the\n",
    "label in your data set to get an intution of its\n",
    "overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML WITH PYTHON\n",
    "#Format\n",
    "from sklearn.family import Model\n",
    "\n",
    "#linear_model= FAMILY of models\n",
    "#LinearRegression is ESTIMATOR OBJECT(model itself)\n",
    "#Next Step is to INSTANCIATE MODEL(or Estimator)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Estimator Parameters: all the parms of an estimator\n",
    "can be set when it is instanciated and have suitable\n",
    "defualt values. Use SHIFT+TAB to check for possible\n",
    "params\n",
    "\n",
    "#For ex\n",
    "#You can instanciate a LinReg model by specifying \n",
    "#param to be NORMALIZE=TRUE:\n",
    "model = LinearRegression(normalize=True)\n",
    "print(model)\n",
    "\n",
    "#Then after printing you can check our params that\n",
    " #were defualted to model:\n",
    "LinearRegression(copy_X=True, fit_interecept=True,\n",
    "                normalize=True)\n",
    "\n",
    "#You dont have to specify normalize=True, these\n",
    "are just additional params used to tune model to\n",
    "something more specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "#Lets make some Fake data\n",
    "import numpy as np\n",
    "#Split data into Train/Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X are FEATURES\n",
    "#y is LABEL for each of the feature rows\n",
    "X, y = np.arange(10).reshape((5,2)), range(5)\n",
    "print(X)\n",
    "print(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 5],\n",
       "       [2, 3],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Train Test Split pass in X and Y and pass in \n",
    "#test size\n",
    "#You can use TrainTest split on feautures and \n",
    "#Labels and SKLEARN will output Trainign and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 4]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [6, 7]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we split data we can train/fit on the \n",
    " #TRAINING DATA\n",
    "#Done thru MODEL.FIT\n",
    "#X TRAIN = FEATURES of DATA\n",
    "#Y_TRAIN=TRAINING LABELS\n",
    "model.fit(X_train,y_train)\n",
    "#Now model has been fit and. trained on training data\n",
    "#Model is ready to predict labels or values on the\n",
    "#test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEt PREDICTED VALUES using PREDICT METHOD\n",
    "#X_TEST=TEST FEAUTURES\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then can evaluate by comparing predictions with\n",
    "correct values\n",
    "#Evaluation method depned on what sort of ML algorithm\n",
    "we use(eg Regression, Classification, Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For ALL ESTIMATORS:\n",
    "-Model.fit(): for training data\n",
    " \n",
    "-for Supervised Leaning, this accepts 2 arguments:\n",
    "  the data X and labels y(eg model.fit(X,y))\n",
    "\n",
    "-For Unsupervised Leaning: this accepts only one \n",
    "  sugmets the data X(ig model.fit(X))\n",
    "\n",
    "#For SUPERVIDES ESTIMATORS\n",
    "-model.predict(): given a trained model, predict the\n",
    "    lael of a new set of data. This method accepts\n",
    "    only one argument, the new data X_new\n",
    "    eg(model.predict(X_new)), and returns the learned\n",
    "    label for each object in the array\n",
    "    \n",
    "    \n",
    " SEE VIDEO for rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
